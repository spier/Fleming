{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Project Fleming","text":"<p>Introducing Project Fleming: Originally devised for Shell, this model-agnostic, AI-powered Discovery Tool harnesses open source models from Hugging Face on Small CPU Clusters on Databricks. We invite contributors and users alike to join us in enabling Discovery for all organizations looking to power their Inner Source initiatives or use Artificial Intelligence for specialized Discovery tasks.</p> <p>Inner Source refers to the deployment of Open Source practices of code and information sharing within a single organization. By sharing, we reduce duplication, free up cognitive space for innovation, and increase the speed of delivery.</p> <p>However, simply setting repositories to \u201copen\u201d is not enough to ensure effective sharing. Traditional search tools often have a narrow focus and rely on specificity.</p> <p>The solution is a Discovery tool, allowing developers to find unexpected yet helpful solutions and information. Leveraging the power of Artificial Intelligence, this tool is designed to be resource-efficient, using clever programming techniques to serve models with Databricks on a CPU cluster instead of a GPU one.</p> <p>Join us in contributing to Project Fleming and help drive innovation and efficiency through enhanced discovery and sharing.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/11/19/introducing-project-fleming-enhancing-code-reuse-and-efficiency-with-ai-discovery/","title":"Introducing Project Fleming: Enhancing Code Reuse and Efficiency with AI Discovery","text":"<p>Today, Shell\u2019s Inner Source Enablement Team has released key packages for a new AI-powered Discovery Tool. These packages are available for Open Source use.</p> <p>The Discovery Tool was created to address a specific Inner Source challenge: while setting a code repository to \u201copen\u201d is a good start, integrating it into a workflow using natural language search takes it further. Discovery is not just about finding what you expect; it\u2019s also about uncovering what you may not have anticipated. By leveraging AI, we make this possible. The tool can search for code as well as Stack Overflow entries. Our goal is to ensure that no developer leaves empty-handed, ultimately enhancing efficiency. Already, the tool has helped developers identify opportunities for code reuse.</p> <p>The idea for this tool originated from PhD research at Loughborough University, which demonstrated how powerful AI could be in classifying information, even when it was in an unstructured state.</p> <p>This tool would not have come to life without the skill, dedication, and creativity of the Inner Source Enablement Team and all those who supported us. Innovation rarely happens in isolation, and this was no exception. The team not only brought the concept to life but made it efficient, for example, by running it on a CPU instead of a GPU setup. Furthermore, we have striven to make it model-agnostic, thus allowing users to integrate the LLM of their choice.</p> <p>Specific thanks go to the following individuals:</p> <p>Shell</p> <ul> <li>Benedict Butcher</li> <li>Amber Rigg</li> <li>Tugce Ozberk Yener</li> <li>Oliver Carr</li> <li>Gabriel Molina</li> <li>Tom Lewis</li> <li>Anusha Modwal</li> <li>Michelle Bhaskaran</li> <li>Herman Kruis</li> <li>Niranjan Girhe</li> <li>Neethi Mary Regi</li> <li>Namitha Raveendranathan.</li> </ul> <p>Loughborough University</p> <ul> <li>Professor Jenny Harding</li> <li>Dr Diana Segura-Velandia</li> </ul> <p>We also could not have achieved this without management support from Bryce Bartmann, Dan Jeavons, Warren Harding, Adam Jordan and Karina Fernandez.</p> <p>We encourage all colleagues and potential collaborators interested in Inner Source and Open Source to explore the tool. We welcome your contributions as we continue its development. We also look forward to unexpected insights and new features that others may add.</p> <p>As always, we value your feedback - feedback is a gift.</p>"},{"location":"code-reference/CorpusTextCreation/","title":"CorpusTextCreation","text":""},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation","title":"<code>CorpusTextCreation</code>","text":"<p>Class to create the corpus txt file for the semantic search model from a dataframe.</p> <p>The class contains the following methods:</p> <ol> <li>concat_columns: Concatenate the columns to create the corpus from the dataframe. This will take all the columns in the dataframe and concatenate them to create the corpus.</li> <li>write_corpus_to_file: Write the corpus to a file from the concatenated columns.</li> </ol>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation--example","title":"Example","text":"<pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\ncorpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\ncorpus_file_path = \"/tmp/search_corpus.txt\"\n\ncorpus_creation = CorpusCreation(corpus_df, corpus_file_path)\ncorpus = corpus_creation.concat_columns()\ncorpus_creation.write_corpus_to_file(corpus)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>corpus_df</code> <code>df</code> <p>Source dataframe of the corpus</p> required <code>corpus_file_path</code> <code>str</code> <p>File path to write the corpus</p> required Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>class CorpusTextCreation:\n    \"\"\"\n    Class to create the corpus txt file for the semantic search model from a dataframe.\n\n    The class contains the following methods:\n\n    1. concat_columns: Concatenate the columns to create the corpus from the dataframe. This will take all the columns in the dataframe and concatenate them to create the corpus.\n    2. write_corpus_to_file: Write the corpus to a file from the concatenated columns.\n\n      Example\n    --------\n    ```python\n\n    from fleming.discovery.corpus_creation import CorpusCreation\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\n    corpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\n    corpus_file_path = \"/tmp/search_corpus.txt\"\n\n    corpus_creation = CorpusCreation(corpus_df, corpus_file_path)\n    corpus = corpus_creation.concat_columns()\n    corpus_creation.write_corpus_to_file(corpus)\n\n    ```\n\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        corpus_df (df): Source dataframe of the corpus\n        corpus_file_path (str): File path to write the corpus\n\n    \"\"\"\n\n    spark: SparkSession\n    corpus_df: DataFrame\n    corpus_file_path: str\n\n    def __init__(\n        self, spark: SparkSession, corpus_df: DataFrame, corpus_file_path: str\n    ) -&gt; None:\n        self.spark = spark\n        self.corpus_df = corpus_df\n        self.corpus_file_path = corpus_file_path\n\n    def concat_columns(self) -&gt; list:\n        \"\"\"\n        Concatenate the columns to create the corpus\n\n        Parameters:\n        None\n\n        Returns:\n        corpus(list): List of concatenated columns\n\n        \"\"\"\n        df = self.corpus_df.withColumn(\n            \"ConcatColumns\", concat_ws(\" \", *self.corpus_df.columns)\n        )\n\n        corpus = [row[\"ConcatColumns\"] for row in df.collect()]\n\n        return corpus\n\n    def write_corpus_to_file(self, corpus) -&gt; None:\n        \"\"\"\n        Write the corpus to a file\n\n        Parameters:\n        corpus(list): List of concatenated columns\n\n        Returns:\n        None\n        \"\"\"\n        with open(self.corpus_file_path, \"w\") as file:\n            for sentence in corpus:\n                try:\n                    file.write(sentence + \"\\n\")\n                    print(sentence)\n                except Exception as e:\n                    logging.exception(str(e))\n                    raise e\n</code></pre>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation.concat_columns","title":"<code>concat_columns()</code>","text":"<p>Concatenate the columns to create the corpus</p> <p>Parameters: None</p> <p>Returns: corpus(list): List of concatenated columns</p> Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>def concat_columns(self) -&gt; list:\n    \"\"\"\n    Concatenate the columns to create the corpus\n\n    Parameters:\n    None\n\n    Returns:\n    corpus(list): List of concatenated columns\n\n    \"\"\"\n    df = self.corpus_df.withColumn(\n        \"ConcatColumns\", concat_ws(\" \", *self.corpus_df.columns)\n    )\n\n    corpus = [row[\"ConcatColumns\"] for row in df.collect()]\n\n    return corpus\n</code></pre>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation.write_corpus_to_file","title":"<code>write_corpus_to_file(corpus)</code>","text":"<p>Write the corpus to a file</p> <p>Parameters: corpus(list): List of concatenated columns</p> <p>Returns: None</p> Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>def write_corpus_to_file(self, corpus) -&gt; None:\n    \"\"\"\n    Write the corpus to a file\n\n    Parameters:\n    corpus(list): List of concatenated columns\n\n    Returns:\n    None\n    \"\"\"\n    with open(self.corpus_file_path, \"w\") as file:\n        for sentence in corpus:\n            try:\n                file.write(sentence + \"\\n\")\n                print(sentence)\n            except Exception as e:\n                logging.exception(str(e))\n                raise e\n</code></pre>"},{"location":"code-reference/ModelQuery/","title":"ModelQuery","text":""},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery","title":"<code>ModelQuery</code>","text":"<p>A class which allows for querying a model serving endpoint on databricks.</p> <p>This class is used to query a model serving endpoint on databricks with a dataset.</p>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery--example","title":"Example:","text":"<pre><code>url = \"https://example.com/model_endpoint\"\ntoken = \"your_auth_token\"\n\n# Create an instance of ModelQuery\nmodel_query = ModelQuery(url, token)\n\n# Example dataset\ndataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\ntry:\n    # Score the model using the dataset\n    response = model_query.score_model(dataset)\n    print(response)\nexcept requests.exceptions.HTTPError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the model serving endpoint.</p> required <code>token</code> <code>str</code> <p>The authorization token for the model serving endpoint.</p> required Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>class ModelQuery:\n    \"\"\"\n    A class which allows for querying a model serving endpoint on databricks.\n\n    This class is used to query a model serving endpoint on databricks with a dataset.\n\n    Example:\n    --------\n    ```python\n\n    url = \"https://example.com/model_endpoint\"\n    token = \"your_auth_token\"\n\n    # Create an instance of ModelQuery\n    model_query = ModelQuery(url, token)\n\n    # Example dataset\n    dataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\n    try:\n        # Score the model using the dataset\n        response = model_query.score_model(dataset)\n        print(response)\n    except requests.exceptions.HTTPError as e:\n        print(f\"Error: {str(e)}\")\n\n    ```\n\n    Parameters:\n        url (str): The URL of the model serving endpoint.\n        token (str): The authorization token for the model serving endpoint.\n    \"\"\"\n\n    url: str\n    token: str\n\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def create_tf_serving_json(self, data):\n        \"\"\"\n        Creates a JSON object for TensorFlow serving.\n\n        Parameters:\n            data (Union[dict, pd.DataFrame, np.ndarray]): The input data.\n\n        Returns:\n            dict: The JSON object for TensorFlow serving.\n        \"\"\"\n        return {\n            \"inputs\": (\n                {name: data[name].tolist() for name in data.keys()}\n                if isinstance(data, dict)\n                else data.tolist()\n            )\n        }\n\n    def score_model(self, dataset):\n        \"\"\"\n        Scores the model using the provided dataset.\n\n        Parameters:\n            dataset (Union[pd.DataFrame, np.ndarray]): The dataset to be scored.\n\n        Returns:\n            dict: The response JSON from the model serving endpoint.\n\n        Raises:\n            requests.exceptions.HTTPError: If the request to the model serving endpoint fails.\n        \"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n        ds_dict = (\n            {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n            if isinstance(dataset, pd.DataFrame)\n            else self.create_tf_serving_json(dataset)\n        )\n        data_json = json.dumps(ds_dict, allow_nan=True)\n        response = requests.request(\n            method=\"POST\", headers=headers, url=self.url, data=data_json\n        )\n        if response.status_code != 200:\n            raise requests.exceptions.HTTPError(\n                f\"Request failed with status {response.status_code}, {response.text}\"\n            )\n        return response.json()\n</code></pre>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery.create_tf_serving_json","title":"<code>create_tf_serving_json(data)</code>","text":"<p>Creates a JSON object for TensorFlow serving.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, DataFrame, ndarray]</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The JSON object for TensorFlow serving.</p> Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>def create_tf_serving_json(self, data):\n    \"\"\"\n    Creates a JSON object for TensorFlow serving.\n\n    Parameters:\n        data (Union[dict, pd.DataFrame, np.ndarray]): The input data.\n\n    Returns:\n        dict: The JSON object for TensorFlow serving.\n    \"\"\"\n    return {\n        \"inputs\": (\n            {name: data[name].tolist() for name in data.keys()}\n            if isinstance(data, dict)\n            else data.tolist()\n        )\n    }\n</code></pre>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery.score_model","title":"<code>score_model(dataset)</code>","text":"<p>Scores the model using the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[DataFrame, ndarray]</code> <p>The dataset to be scored.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The response JSON from the model serving endpoint.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the request to the model serving endpoint fails.</p> Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>def score_model(self, dataset):\n    \"\"\"\n    Scores the model using the provided dataset.\n\n    Parameters:\n        dataset (Union[pd.DataFrame, np.ndarray]): The dataset to be scored.\n\n    Returns:\n        dict: The response JSON from the model serving endpoint.\n\n    Raises:\n        requests.exceptions.HTTPError: If the request to the model serving endpoint fails.\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {self.token}\",\n        \"Content-Type\": \"application/json\",\n    }\n    ds_dict = (\n        {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n        if isinstance(dataset, pd.DataFrame)\n        else self.create_tf_serving_json(dataset)\n    )\n    data_json = json.dumps(ds_dict, allow_nan=True)\n    response = requests.request(\n        method=\"POST\", headers=headers, url=self.url, data=data_json\n    )\n    if response.status_code != 200:\n        raise requests.exceptions.HTTPError(\n            f\"Request failed with status {response.status_code}, {response.text}\"\n        )\n    return response.json()\n</code></pre>"},{"location":"code-reference/ModelServe/","title":"ModelServe","text":""},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe","title":"<code>ModelServe</code>","text":"<p>A class which allows for creating a model serving endpoint on databricks.</p>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe--example","title":"Example:","text":"<pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"aidiscoverytool\"\nprint(f'Endpoint name: {endpoint_name}')\n\n# Name of the registered MLflow model\nmodel_name = \"BERT_Semantic_Search\"\nprint(f'Model name: {model_name}')\n\n# Get the latest version of the MLflow model\nlatest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\nmodel_version = latest_version.version\nprint(f'Model version: {model_version}')\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\nworkload_type = \"CPU\"\nprint(f'Workload type: {workload_type}')\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\nprint(f'Workload size: {workload_size}')\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False\nprint(f'Scale to zero: {scale_to_zero}')\n\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\nmodel_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the model serving endpoint.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to be served.</p> required <code>workload_type</code> <code>str</code> <p>The type of compute to be used for the endpoint.</p> required <code>workload_size</code> <code>str</code> <p>The scale-out size of the compute.</p> required <code>scale_to_zero</code> <code>bool</code> <p>Whether to scale the compute to zero when not in use.</p> required <code>API_ROOT</code> <code>str</code> <p>The API root of the Databricks workspace.</p> <code>None</code> <code>API_TOKEN</code> <code>str</code> <p>The API token of the Databricks workspace.</p> <code>None</code> Source code in <code>src/fleming/discovery/model_serve.py</code> <pre><code>class ModelServe:\n    \"\"\"\n    A class which allows for creating a model serving endpoint on databricks.\n\n    Example:\n    --------\n    ```python\n\n    from fleming.discovery.corpus_creation import CorpusCreation\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n    # Set the name of the MLflow endpoint\n    endpoint_name = \"aidiscoverytool\"\n    print(f'Endpoint name: {endpoint_name}')\n\n    # Name of the registered MLflow model\n    model_name = \"BERT_Semantic_Search\"\n    print(f'Model name: {model_name}')\n\n    # Get the latest version of the MLflow model\n    latest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\n    model_version = latest_version.version\n    print(f'Model version: {model_version}')\n\n    # Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\n    workload_type = \"CPU\"\n    print(f'Workload type: {workload_type}')\n\n    # Specify the scale-out size of compute (Small, Medium, Large, etc.)\n    workload_size = \"Small\"\n    print(f'Workload size: {workload_size}')\n\n    # Specify Scale to Zero(only supported for CPU endpoints)\n    scale_to_zero = False\n    print(f'Scale to zero: {scale_to_zero}')\n\n    API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n    API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n    model_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\n    model_serve.deploy_endpoint()\n\n    ```\n\n    Parameters:\n        endpoint_name (str): The name of the model serving endpoint.\n        model_name (str): The name of the model to be served.\n        workload_type (str): The type of compute to be used for the endpoint.\n        workload_size (str): The scale-out size of the compute.\n        scale_to_zero (bool): Whether to scale the compute to zero when not in use.\n        API_ROOT (str): The API root of the Databricks workspace.\n        API_TOKEN (str): The API token of the Databricks workspace.\n    \"\"\"\n\n    spark: SparkSession\n    endpoint_name: str\n    model_name: str\n    workload_type: str\n    workload_size: str\n    scale_to_zero: bool\n    API_ROOT: str\n    API_TOKEN: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        endpoint_name: str,\n        model_name: str,\n        workload_type: str,\n        workload_size: str,\n        scale_to_zero: str,\n        API_ROOT: str = None,\n        API_TOKEN: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.endpoint_name = endpoint_name\n        self.model_name = model_name\n        self.workload_type = workload_type\n        self.workload_size = workload_size\n        self.scale_to_zero = scale_to_zero\n        self.API_ROOT = API_ROOT\n        self.API_TOKEN = API_TOKEN\n\n    def deploy_endpoint(self) -&gt; None:\n        \"\"\"\n        Create the model serving endpoint on Databricks\n\n        \"\"\"\n\n        try:\n            client = get_deploy_client(\"databricks\")\n            client.create_endpoint(\n                name=self.endpoint_name,\n                config={\n                    \"served_entities\": [\n                        {\n                            \"name\": self.model_name,\n                            \"entity_name\": self.model_name,\n                            \"entity_version\": MlflowClient()\n                            .get_registered_model(self.model_name)\n                            .latest_versions[1]\n                            .version,\n                            \"workload_type\": self.workload_type,\n                            \"workload_size\": self.workload_size,\n                            \"scale_to_zero_enabled\": self.scale_to_zero,\n                        }\n                    ],\n                    \"traffic_config\": {\n                        \"routes\": [\n                            {\n                                \"served_model_name\": self.model_name,\n                                \"traffic_percentage\": 100,\n                            }\n                        ]\n                    },\n                },\n            )\n        except requests.exceptions.RequestException as e:\n            put_url = \"/api/2.0/serving-endpoints/{}/config\".format(self.endpoint_name)\n            put_url\n\n            data = {\n                \"name\": self.endpoint_name,\n                \"config\": {\n                    \"served_entities\": [\n                        {\n                            \"name\": self.model_name,\n                            \"entity_name\": self.model_name,\n                            \"entity_version\": max(\n                                MlflowClient().get_latest_versions(self.model_name),\n                                key=lambda v: v.version,\n                            ).version,\n                            \"workload_type\": self.workload_type,\n                            \"workload_size\": self.workload_size,\n                            \"scale_to_zero_enabled\": self.scale_to_zero,\n                        }\n                    ],\n                    \"traffic_config\": {\n                        \"routes\": [\n                            {\n                                \"served_model_name\": self.model_name,\n                                \"traffic_percentage\": 100,\n                            }\n                        ]\n                    },\n                },\n            }\n\n            headers = {\n                \"Context-Type\": \"text/json\",\n                \"Authorization\": f\"Bearer {self.API_TOKEN}\",\n            }\n\n            response = requests.put(\n                url=f\"{self.API_ROOT}{put_url}\", json=data[\"config\"], headers=headers\n            )\n\n            if response.status_code != 200:\n                raise requests.exceptions.RequestException(\n                    f\"Request failed with status {response.status_code}, {response.text}\"\n                )\n\n            return response.json()\n            raise\n</code></pre>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe.deploy_endpoint","title":"<code>deploy_endpoint()</code>","text":"<p>Create the model serving endpoint on Databricks</p> Source code in <code>src/fleming/discovery/model_serve.py</code> <pre><code>def deploy_endpoint(self) -&gt; None:\n    \"\"\"\n    Create the model serving endpoint on Databricks\n\n    \"\"\"\n\n    try:\n        client = get_deploy_client(\"databricks\")\n        client.create_endpoint(\n            name=self.endpoint_name,\n            config={\n                \"served_entities\": [\n                    {\n                        \"name\": self.model_name,\n                        \"entity_name\": self.model_name,\n                        \"entity_version\": MlflowClient()\n                        .get_registered_model(self.model_name)\n                        .latest_versions[1]\n                        .version,\n                        \"workload_type\": self.workload_type,\n                        \"workload_size\": self.workload_size,\n                        \"scale_to_zero_enabled\": self.scale_to_zero,\n                    }\n                ],\n                \"traffic_config\": {\n                    \"routes\": [\n                        {\n                            \"served_model_name\": self.model_name,\n                            \"traffic_percentage\": 100,\n                        }\n                    ]\n                },\n            },\n        )\n    except requests.exceptions.RequestException as e:\n        put_url = \"/api/2.0/serving-endpoints/{}/config\".format(self.endpoint_name)\n        put_url\n\n        data = {\n            \"name\": self.endpoint_name,\n            \"config\": {\n                \"served_entities\": [\n                    {\n                        \"name\": self.model_name,\n                        \"entity_name\": self.model_name,\n                        \"entity_version\": max(\n                            MlflowClient().get_latest_versions(self.model_name),\n                            key=lambda v: v.version,\n                        ).version,\n                        \"workload_type\": self.workload_type,\n                        \"workload_size\": self.workload_size,\n                        \"scale_to_zero_enabled\": self.scale_to_zero,\n                    }\n                ],\n                \"traffic_config\": {\n                    \"routes\": [\n                        {\n                            \"served_model_name\": self.model_name,\n                            \"traffic_percentage\": 100,\n                        }\n                    ]\n                },\n            },\n        }\n\n        headers = {\n            \"Context-Type\": \"text/json\",\n            \"Authorization\": f\"Bearer {self.API_TOKEN}\",\n        }\n\n        response = requests.put(\n            url=f\"{self.API_ROOT}{put_url}\", json=data[\"config\"], headers=headers\n        )\n\n        if response.status_code != 200:\n            raise requests.exceptions.RequestException(\n                f\"Request failed with status {response.status_code}, {response.text}\"\n            )\n\n        return response.json()\n        raise\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/","title":"ModelTrainRegister","text":""},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel","title":"<code>SemanticSearchModel</code>","text":"<p>               Bases: <code>PythonModel</code></p> <p>A class representing a semantic search model.</p> <p>This class is used to perform semantic search over a corpus of sentences using a pre-trained model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The pre-trained model used for encoding sentences.</p> <code>corpus</code> <p>The corpus of sentences used for semantic search.</p> <code>corpus_embeddings</code> <p>The embeddings of the sentences in the corpus.</p> <p>Methods:</p> Name Description <code>load_context</code> <p>Load the model context for inference, including the corpus from a file.</p> <code>predict</code> <p>Perform semantic search over the corpus and return the most relevant results.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>class SemanticSearchModel(PythonModel):\n    \"\"\"\n    A class representing a semantic search model.\n\n    This class is used to perform semantic search over a corpus of sentences using a pre-trained model.\n\n    Attributes:\n        model: The pre-trained model used for encoding sentences.\n        corpus: The corpus of sentences used for semantic search.\n        corpus_embeddings: The embeddings of the sentences in the corpus.\n\n    Methods:\n        load_context: Load the model context for inference, including the corpus from a file.\n        predict: Perform semantic search over the corpus and return the most relevant results.\n    \"\"\"\n\n    def load_context(self, context):\n        \"\"\"\n        Load the model context for inference, including the corpus from a file.\n        \"\"\"\n        try:\n            self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n            # Load the corpus from the specified file\n            corpus_file = context.artifacts[\"corpus_file\"]\n            with open(corpus_file) as file:\n                self.corpus = file.read().splitlines()\n\n            self.corpus_embeddings = torch.load(\n                context.artifacts[\"corpus_embedding_file\"]\n            )\n\n        except Exception as e:\n            raise ValueError(f\"Error loading model and corpus: {e}\")\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        Predict method to perform semantic search over the corpus.\n\n        Args:\n            context: The context object containing the model artifacts.\n            model_input: The input data for performing semantic search.\n            params: Optional parameters for controlling the search behavior.\n\n        Returns:\n            A list of tuples containing the most relevant sentences from the corpus and their similarity scores.\n        \"\"\"\n\n        if isinstance(model_input, pd.DataFrame):\n            if model_input.shape[1] != 1:\n                raise ValueError(\"DataFrame input must have exactly one column.\")\n            model_input = model_input.iloc[0, 0]\n        elif isinstance(model_input, dict):\n            model_input = model_input.get(\"sentence\")\n            if model_input is None:\n                raise ValueError(\n                    \"The input dictionary must have a key named 'sentence'.\"\n                )\n        else:\n            raise TypeError(\n                f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n            )\n\n        # Encode the query\n        query_embedding = self.model.encode(model_input, convert_to_tensor=True)\n\n        # Compute cosine similarity scores\n        cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings).cpu()[0]\n\n        # Determine the number of top results to return\n        top_k = params.get(\"top_k\", 3) if params else 3  # Default to 3 if not specified\n\n        _ = (\n            params.get(\"minimum_relevancy\", 0.4) if params else 0.4\n        )  # Default to 0.4 if not specified\n\n        # Get the top_k most similar sentences from the corpus\n        top_results = np.argsort(cos_scores, axis=0)[-top_k:]\n\n        # Prepare the initial results list\n        initial_results = [\n            (self.corpus[idx], cos_scores[idx].item()) for idx in reversed(top_results)\n        ]\n\n        # Filter the results based on the minimum relevancy threshold\n        filtered_results = [result for result in initial_results if result[1] &gt;= 0]\n\n        # If all results are below the threshold, issue a warning and return the top result\n        if not filtered_results:\n            warnings.warn(\n                \"All top results are below the minimum relevancy threshold. \"\n                \"Returning the highest match instead.\",\n                RuntimeWarning,\n            )\n            return [initial_results[0]]\n        else:\n            return filtered_results\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel.load_context","title":"<code>load_context(context)</code>","text":"<p>Load the model context for inference, including the corpus from a file.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def load_context(self, context):\n    \"\"\"\n    Load the model context for inference, including the corpus from a file.\n    \"\"\"\n    try:\n        self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n        # Load the corpus from the specified file\n        corpus_file = context.artifacts[\"corpus_file\"]\n        with open(corpus_file) as file:\n            self.corpus = file.read().splitlines()\n\n        self.corpus_embeddings = torch.load(\n            context.artifacts[\"corpus_embedding_file\"]\n        )\n\n    except Exception as e:\n        raise ValueError(f\"Error loading model and corpus: {e}\")\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel.predict","title":"<code>predict(context, model_input, params=None)</code>","text":"<p>Predict method to perform semantic search over the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>The context object containing the model artifacts.</p> required <code>model_input</code> <p>The input data for performing semantic search.</p> required <code>params</code> <p>Optional parameters for controlling the search behavior.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of tuples containing the most relevant sentences from the corpus and their similarity scores.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def predict(self, context, model_input, params=None):\n    \"\"\"\n    Predict method to perform semantic search over the corpus.\n\n    Args:\n        context: The context object containing the model artifacts.\n        model_input: The input data for performing semantic search.\n        params: Optional parameters for controlling the search behavior.\n\n    Returns:\n        A list of tuples containing the most relevant sentences from the corpus and their similarity scores.\n    \"\"\"\n\n    if isinstance(model_input, pd.DataFrame):\n        if model_input.shape[1] != 1:\n            raise ValueError(\"DataFrame input must have exactly one column.\")\n        model_input = model_input.iloc[0, 0]\n    elif isinstance(model_input, dict):\n        model_input = model_input.get(\"sentence\")\n        if model_input is None:\n            raise ValueError(\n                \"The input dictionary must have a key named 'sentence'.\"\n            )\n    else:\n        raise TypeError(\n            f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n        )\n\n    # Encode the query\n    query_embedding = self.model.encode(model_input, convert_to_tensor=True)\n\n    # Compute cosine similarity scores\n    cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings).cpu()[0]\n\n    # Determine the number of top results to return\n    top_k = params.get(\"top_k\", 3) if params else 3  # Default to 3 if not specified\n\n    _ = (\n        params.get(\"minimum_relevancy\", 0.4) if params else 0.4\n    )  # Default to 0.4 if not specified\n\n    # Get the top_k most similar sentences from the corpus\n    top_results = np.argsort(cos_scores, axis=0)[-top_k:]\n\n    # Prepare the initial results list\n    initial_results = [\n        (self.corpus[idx], cos_scores[idx].item()) for idx in reversed(top_results)\n    ]\n\n    # Filter the results based on the minimum relevancy threshold\n    filtered_results = [result for result in initial_results if result[1] &gt;= 0]\n\n    # If all results are below the threshold, issue a warning and return the top result\n    if not filtered_results:\n        warnings.warn(\n            \"All top results are below the minimum relevancy threshold. \"\n            \"Returning the highest match instead.\",\n            RuntimeWarning,\n        )\n        return [initial_results[0]]\n    else:\n        return filtered_results\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister","title":"<code>ModelTrainRegister</code>","text":"<p>A class to train and register a semantic search model.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister--example","title":"Example:","text":"<pre><code>from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\nmodel_directory = \"/tmp/BERT_Semantic_Search_model\"\ncorpus_file = \"/tmp/search_corpus.txt\"\ncorpus_embedding_file = '/tmp/corpus_embedding.pt'\n\nmodel_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n# Register the model\nsemantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\nmodel_developer.register_model(semantic_search_model)\n\n# Embed the corpus\nmodel_developer.embed_corpus()\n\n# Define parameters and artifacts\nparameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\ninput_example = [\"Innersource best practices\"]\ntest_output = [\"match 1\", \"match 2\"]\nsignature = infer_signature(input_example, test_output, params=parameters)\nartifacts = {\n    \"model_path\": model_directory,\n    \"corpus_file\": corpus_file,\n    \"corpus_embedding_file\": corpus_embedding_file\n}\nunique_model_name = \"semantic_search_model\"\n\n# Create and serve the model\nexperiment_location = \"/path/to/experiment\"\nmodel_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister--parameters","title":"Parameters:","text":"<p>model_directory (str): The directory to save the trained model. corpus_file (str): The file containing the corpus of sentences. corpus_embedding_file (str): The file to save the embeddings of the corpus. semantic_search_model (str): The pre-trained model to use for semantic search.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>class ModelTrainRegister:\n    \"\"\"\n    A class to train and register a semantic search model.\n\n    Example:\n    --------\n    ```python\n\n    from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n    model_directory = \"/tmp/BERT_Semantic_Search_model\"\n    corpus_file = \"/tmp/search_corpus.txt\"\n    corpus_embedding_file = '/tmp/corpus_embedding.pt'\n\n    model_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n    # Register the model\n    semantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\n    model_developer.register_model(semantic_search_model)\n\n    # Embed the corpus\n    model_developer.embed_corpus()\n\n    # Define parameters and artifacts\n    parameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\n    input_example = [\"Innersource best practices\"]\n    test_output = [\"match 1\", \"match 2\"]\n    signature = infer_signature(input_example, test_output, params=parameters)\n    artifacts = {\n        \"model_path\": model_directory,\n        \"corpus_file\": corpus_file,\n        \"corpus_embedding_file\": corpus_embedding_file\n    }\n    unique_model_name = \"semantic_search_model\"\n\n    # Create and serve the model\n    experiment_location = \"/path/to/experiment\"\n    model_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n    ```\n\n    Parameters:\n    -----------\n    model_directory (str): The directory to save the trained model.\n    corpus_file (str): The file containing the corpus of sentences.\n    corpus_embedding_file (str): The file to save the embeddings of the corpus.\n    semantic_search_model (str): The pre-trained model to use for semantic search.\n    \"\"\"\n\n    spark: SparkSession\n    model_directory: str\n    corpus_file: str\n    corpus_embedding_file: str\n    semantic_search_model: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        model_directory: str,\n        corpus_file: str,\n        corpus_embedding_file: str,\n        semantic_search_model: str,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ModelDeveloper class.\n\n        Parameters:\n        -----------\n        spark : SparkSession\n        model_directory : str\n            The directory to save the trained model.\n        corpus_file : str\n            The file containing the corpus of sentences.\n        corpus_embedding_file : str\n            The file to save the embeddings of the corpus.\n        semantic_search_model : str\n            The pre-trained model to use for semantic search.\n        \"\"\"\n        self.model_directory = model_directory\n        self.corpus_file = corpus_file\n        self.corpus_embedding_file = corpus_embedding_file\n        self.semantic_search_model = semantic_search_model\n\n    def register_model(self) -&gt; None:\n        \"\"\"\n        Register the pre-trained model.\n\n        \"\"\"\n        model = SentenceTransformer(self.semantic_search_model)\n        model.save(self.model_directory)\n\n    def embed_corpus(self) -&gt; None:\n        \"\"\"\n        Embed the corpus of sentences using the pre-trained model.\n        \"\"\"\n        model = SentenceTransformer.load(self.model_directory)\n\n        with open(self.corpus_file) as file:\n            corpus = file.read().splitlines()\n\n        corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n        torch.save(corpus_embeddings, self.corpus_embedding_file)\n\n    def create_registered_model(\n        self,\n        unique_model_name: str,\n        input_example: list,\n        signature: object,\n        artifacts: str,\n        experiment_location: str,\n    ) -&gt; None:\n        \"\"\"\n        Create and serve the semantic search model.\n\n        Parameters:\n        -----------\n        unique_model_name : str\n            The unique name for the model.\n        input_example : list\n            An example input for the model.\n        signature : object\n            The signature object for the model.\n        artifacts : dict\n            The artifacts required for the model.\n        experiment_location : str\n            The location to store the experiment.\n        \"\"\"\n        mlflow.set_experiment(experiment_location)\n\n        with mlflow.start_run() as run:\n            model_info = mlflow.pyfunc.log_model(\n                unique_model_name,\n                python_model=SemanticSearchModel(),\n                input_example=input_example,\n                signature=signature,\n                artifacts=artifacts,\n                pip_requirements=[\"sentence_transformers\", \"numpy\"],\n            )\n            _ = run.info.run_id\n\n            model_uri = model_info.model_uri\n\n            mlflow.register_model(model_uri=model_uri, name=unique_model_name)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.__init__","title":"<code>__init__(spark, model_directory, corpus_file, corpus_embedding_file, semantic_search_model)</code>","text":"<p>Initialize the ModelDeveloper class.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.__init__--parameters","title":"Parameters:","text":"<p>spark : SparkSession model_directory : str     The directory to save the trained model. corpus_file : str     The file containing the corpus of sentences. corpus_embedding_file : str     The file to save the embeddings of the corpus. semantic_search_model : str     The pre-trained model to use for semantic search.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    model_directory: str,\n    corpus_file: str,\n    corpus_embedding_file: str,\n    semantic_search_model: str,\n) -&gt; None:\n    \"\"\"\n    Initialize the ModelDeveloper class.\n\n    Parameters:\n    -----------\n    spark : SparkSession\n    model_directory : str\n        The directory to save the trained model.\n    corpus_file : str\n        The file containing the corpus of sentences.\n    corpus_embedding_file : str\n        The file to save the embeddings of the corpus.\n    semantic_search_model : str\n        The pre-trained model to use for semantic search.\n    \"\"\"\n    self.model_directory = model_directory\n    self.corpus_file = corpus_file\n    self.corpus_embedding_file = corpus_embedding_file\n    self.semantic_search_model = semantic_search_model\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.register_model","title":"<code>register_model()</code>","text":"<p>Register the pre-trained model.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def register_model(self) -&gt; None:\n    \"\"\"\n    Register the pre-trained model.\n\n    \"\"\"\n    model = SentenceTransformer(self.semantic_search_model)\n    model.save(self.model_directory)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.embed_corpus","title":"<code>embed_corpus()</code>","text":"<p>Embed the corpus of sentences using the pre-trained model.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def embed_corpus(self) -&gt; None:\n    \"\"\"\n    Embed the corpus of sentences using the pre-trained model.\n    \"\"\"\n    model = SentenceTransformer.load(self.model_directory)\n\n    with open(self.corpus_file) as file:\n        corpus = file.read().splitlines()\n\n    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n    torch.save(corpus_embeddings, self.corpus_embedding_file)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.create_registered_model","title":"<code>create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)</code>","text":"<p>Create and serve the semantic search model.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.create_registered_model--parameters","title":"Parameters:","text":"<p>unique_model_name : str     The unique name for the model. input_example : list     An example input for the model. signature : object     The signature object for the model. artifacts : dict     The artifacts required for the model. experiment_location : str     The location to store the experiment.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def create_registered_model(\n    self,\n    unique_model_name: str,\n    input_example: list,\n    signature: object,\n    artifacts: str,\n    experiment_location: str,\n) -&gt; None:\n    \"\"\"\n    Create and serve the semantic search model.\n\n    Parameters:\n    -----------\n    unique_model_name : str\n        The unique name for the model.\n    input_example : list\n        An example input for the model.\n    signature : object\n        The signature object for the model.\n    artifacts : dict\n        The artifacts required for the model.\n    experiment_location : str\n        The location to store the experiment.\n    \"\"\"\n    mlflow.set_experiment(experiment_location)\n\n    with mlflow.start_run() as run:\n        model_info = mlflow.pyfunc.log_model(\n            unique_model_name,\n            python_model=SemanticSearchModel(),\n            input_example=input_example,\n            signature=signature,\n            artifacts=artifacts,\n            pip_requirements=[\"sentence_transformers\", \"numpy\"],\n        )\n        _ = run.info.run_id\n\n        model_uri = model_info.model_uri\n\n        mlflow.register_model(model_uri=model_uri, name=unique_model_name)\n</code></pre>"},{"location":"contact/email/","title":"Contact via Email","text":"<p>If you need to contact us directly, we're here to help! You can reach out to us through our official email. Whether you have questions, feedback, or need support, our team is ready to assist you. We value your input and strive to respond promptly to all inquiries. </p> <p>Your communication helps us improve and provide better services, so don't hesitate to get in touch!</p> <p>Emails:</p> <ul> <li>christian.defeo@shell.com</li> </ul>"},{"location":"contact/github/","title":"Reach out on Github","text":"<p>You can interact with us directly on GitHub by raising issues and creating pull requests (PRs). </p> <p>If you encounter any bugs or have suggestions for improvements, simply open an issue to let us know. </p> <p>For those who want to contribute code, you can fork our repository, make your changes, and submit a pull request. We review all contributions and appreciate the community's efforts to enhance our project. Your involvement helps us grow and improve, and we look forward to seeing your contributions!</p> <p>For more on how to contirbute please follow our Contributing guide.</p>"},{"location":"getting-started/calling-the-endpoint/","title":"Query the Endpoint","text":""},{"location":"getting-started/calling-the-endpoint/#documentation","title":"Documentation","text":"<p>Once you have your model serving on a databricks endpoint it is then possible to query this data.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/calling-the-endpoint/#example","title":"Example","text":"<pre><code>url = \"https://example.com/model_endpoint\"\ntoken = \"your_auth_token\"\n\n# Create an instance of ModelQuery\nmodel_query = ModelQuery(url, token)\n\n# Example dataset\ndataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\ntry:\n    # Score the model using the dataset\n    response = model_query.score_model(dataset)\n    print(response)\nexcept requests.exceptions.HTTPError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre>"},{"location":"getting-started/creating-the-corpus/","title":"Create the Corpus","text":""},{"location":"getting-started/creating-the-corpus/#documentation","title":"Documentation","text":"<p>The first step is to create a corpus.txt file, this includes all the valuse you will search over. The below function ingests a dataframe each row being a seperate entry into the corpus. </p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/creating-the-corpus/#example","title":"Example","text":"<pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\ncorpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\ncorpus_file_path = \"/tmp/search_corpus.txt\"\n\ncorpus_creation = CorpusCreation(corpus_df, corpus_file_path)\ncorpus = corpus_creation.concat_columns(df_analytics_cleaned)\ncorpus_creation.write_corpus_to_file(corpus)\n</code></pre>"},{"location":"getting-started/creating-the-model/","title":"Create and Register the Model","text":""},{"location":"getting-started/creating-the-model/#documentation","title":"Documentation","text":"<p>After the corpus.txt file has been created it is now possible to load the corpus to an open source semantic search model and register the model with databricks. Project Fleming embraces open-source and can be used with any open-source model on Hugging Face.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/creating-the-model/#example","title":"Example","text":"<pre><code>from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel    \nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\nmodel_directory = \"/tmp/BERT_Semantic_Search_model\"\ncorpus_file = \"/tmp/search_corpus.txt\"\ncorpus_embedding_file = '/tmp/corpus_embedding.pt'\n\nmodel_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n# Register the model\nsemantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\nmodel_developer.register_model(semantic_search_model)\n\n# Embed the corpus\nmodel_developer.embed_corpus()\n\n# Define parameters and artifacts\nparameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\ninput_example = [\"Innersource best practices\"]\ntest_output = [\"match 1\", \"match 2\"]\nsignature = infer_signature(input_example, test_output, params=parameters)\nartifacts = {\n    \"model_path\": model_directory,\n    \"corpus_file\": corpus_file,\n    \"corpus_embedding_file\": corpus_embedding_file\n}\nunique_model_name = \"semantic_search_model\"\n\n# Create and serve the model\nexperiment_location = \"/path/to/experiment\"\nmodel_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>1) To get started with developing for this project, clone the repository.  <pre><code>    git clone https://github.com/sede-x/Flemming.git\n</code></pre> 2) Open the respository in VS Code, Visual Studio or your preferered code editor.</p> <p>3) Create a new environment using the following command: <pre><code>    micromamba create -f environment.yml\n</code></pre></p> <p>NOTE:  You will need to have conda, python and pip installed to use the command above.</p> <p>4) Activate your newly set up environment using the following command: <pre><code>    micromamba activate \n</code></pre> You are now ready to start developing your own functions. Please remember to follow Felmming's development lifecycle to maintain clarity and efficiency for a fully robust self serving platform. </p> <p>5) For better readability of code is would be useful to enable black and isort on autosave by simply adding this to the VSCode user settings json(Ctrl + Shft + P):</p> <pre><code>    {\n        \"editor.formatOnSave\": true,\n        \"python.formatting.provider\": \"black\",\n        \"python.formatting.blackArgs\": [\n            \"--line-length=119\"\n        ],\n        \"python.sortImports.args\": [\n            \"--profile\",\n            \"black\"\n        ],\n        \"[python]\": {\n            \"editor.codeActionsOnSave\": {\n                \"source.organizeImports\": true\n            }\n        }\n    }\n</code></pre>"},{"location":"getting-started/serving-the-model/","title":"Serve the Registered Model","text":""},{"location":"getting-started/serving-the-model/#documentation","title":"Documentation","text":"<p>After the model has been registered it is now possible to serve the model with the databricks serving endpoint. A unique part of Fleming is that models created can be run on Small CPU Clusters which is both cost and energy efficient.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p> <pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"aidiscoverytool\"\nprint(f'Endpoint name: {endpoint_name}')\n\n# Name of the registered MLflow model\nmodel_name = \"BERT_Semantic_Search\" \nprint(f'Model name: {model_name}')\n\n# Get the latest version of the MLflow model\nmodel_version = MlflowClient().get_registered_model(model_name).latest_versions[1].version \nprint(f'Model version: {model_version}')\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\nworkload_type = \"CPU\"\nprint(f'Workload type: {workload_type}')\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\nprint(f'Workload size: {workload_size}')\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False\nprint(f'Scale to zero: {scale_to_zero}')\n\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\nmodel_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n</code></pre>"},{"location":"releases/fleming/","title":"Releases","text":""},{"location":"releases/fleming/#v001","title":"V0.0.1","text":""},{"location":"releases/fleming/#whats-changed","title":"What's Changed","text":"<ul> <li>Initial commit for src code and documentation. by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/1</li> <li>Update LICENSE.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/2</li> <li>Update CODE_OF_CONDUCT.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/3</li> <li>Update setup.py with Fleming Path by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/4</li> </ul>"},{"location":"releases/fleming/#new-contributors","title":"New Contributors","text":"<ul> <li>@Amber-Rigg made their first contribution in https://github.com/sede-open/Fleming/pull/1</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/commits/v0.01</p>"},{"location":"releases/fleming/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Initial commit for src code and documentation. by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/1</li> <li>Update LICENSE.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/2</li> <li>Update CODE_OF_CONDUCT.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/3</li> <li>Update setup.py with Fleming Path by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/4</li> <li>Feature/00002 - Inclusion of Mkdcos by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/5</li> <li>Update CI for mkdocs by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/6</li> <li>Develop to Main  by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/7</li> <li>Develop to Main  by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/8</li> <li>Update mkdocs.yml by @doctorcdf27 in https://github.com/sede-open/Fleming/pull/9</li> </ul>"},{"location":"releases/fleming/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@Amber-Rigg made their first contribution in https://github.com/sede-open/Fleming/pull/1</li> <li>@doctorcdf27 made their first contribution in https://github.com/sede-open/Fleming/pull/9</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/commits/v0.0.1</p>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}